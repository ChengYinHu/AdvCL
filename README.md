# AdvCL
Code and model for "Adversarial Catoptric Light: An Effective, Stealthy and Robust Physical-World Attack to DNNs" 
<p align='center'>
  <img src='imgs/test.png'>
</p>


## Introduction
Deep neural networks (DNNs) have achieved great success in many tasks. Therefore, it is crucial to evaluate the robustness of advanced DNNs. The traditional methods use stickers as physical perturbations to fool the classifiers, which is difficult to achieve stealthiness and there exists printing loss. Some new types of physical attacks use light beam to perform attacks (e.g., laser, projector), whose optical patterns are artificial rather than natural. 

In this work, we study a new type of physical attack, called adversarial catoptric light (AdvCL), in which adversarial perturbations are generated by common natural phenomena, catoptric light, to achieve stealthy and naturalistic adversarial attacks against advanced DNNs in physical environments.
## Requirements
* python == 3.8
* torch == 1.8.0

## Basic Usage
```sh
python ga_test.py --model resnet50 --dataset your_dataset
```

